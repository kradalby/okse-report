%===================================== CHAP 6 =================================

\chapter{Testing, Not finished}
\label{ch:testing}

\section{Test description}
\label{sec:testing-test_description}

The sections below covers the different test phases. It explains the different tests, as well as discusses the reasoning and usage of the different test methodologies.

\subsection{Unit Testing}
\label{subsec:testing-test_description-unit_testing}

A unit is a portion of source code, sets of one or more computer program modules together with associated control data, usage procedures, and operating procedures. A unit test is a specific test written to testing the validity of said component or components. In Java, a unit is commonly defined as an entire class, but could also be a single object method. A unit test is written to confirm that the methods will always act as intended, even if other parts of the program code is changed. Unit tests were used on the parts of the system containing many logical operations and calculations. Unit tests does not however, exclude the possibility of errors. It only reduces the amount of cases where an error might occur.

\subsection{Integration testing}
\label{subsec:testing-test_description-integration_testing}

Integration testing considers testing where individual modules of the software are grouped. It is a black-box test, in which major parts of the system are tested. The goal of the tests is to make sure that the major parts of the system are working as individual modules. It also ensures that modules are able to communicate with each other.

As each of the protocol implementations and core services of the system are standalone components that communicate with each other, integration testing was vital to the operation of the system. 

\subsection{Profiling}
\label{subsec:testing-test_description-profiling}

Profiling is a way to dynamically analyze the program while it is running. This is done to get an overview of where the resources are used. The method is useful to find bottlenecks and inefficient methods in the program. The software was run as a server instance, and had a possibility for high load on a wide variety of hardware. Profiling was useful to find the parts of the system that had performance issues. It was performed each time some part of the system was completed.

\subsection{System testing}
\label{subsec:testing-test_description-system_testing}

System testing is performed to ensure that the product meets its requirements. It is a black-box test, often performed upon completion of the product. The goal is to go through all the requirements, and ensure that they are met and working as intended. System testing was performed prior to the acceptance test with the customer.

\subsection{Acceptance Testing}
\label{subsec:testing-test_description-acceptance_testing}
Acceptance testing is generally concerned with whether or not the requirements of the system are met in a satisfiable way. Since the system has a lot of functionality that is not visible, the acceptance tests focused on testing the administration interface and to send and receive messages with the implemented applications.
As parts of the acceptance testing, the group performed internal smoke tests of the system after every medium to big component was implemented. General aspects that were tested were requirements compliance, look and feel of the related user interface components, and double checking our interpretation of the customers stated wishes. The reasoning for the smoke tests was to be able to find errors that could impact the result of the final acceptance test.

\section{Test Execution}
\label{subsec:testing-test_execution}

The following sections discusses how tests were planned and executed for each part of the system. The goal is to give a general overview of how tests were conducted and written.

\subsection{Unit testing}
\label{subsec:testing-test_execution-unit_testing}

Initially, the plan was to use test-driven development as often as possible given it was a component we were going to write unit tests for. This proved somewhat difficult. In a lot of cases it was hard to know exactly which units were needed, and as the group learned and evolved, the units changed in scope and behaviour. Thus, tests that initially were written to verify units, were changed later on. This was something the group was well aware of, and intentionally opted to discard TDD where the scope and behaviour of a unit was developed with a trial and error approach. However, that didn't mean that unit tests were discarded as a whole. After these kind of components were finished, unit tests were added to verify their validity, as well as provide a safeguard for errors caused by future alterations.

Units of the system responsible for inter-thread communication, multi-thread task processing and queuing were excluded from unit tests. It proved difficult to write tests for these scenarios, as the group lacked the necessary time and experience to write unit tests for these conditions. The testing of these types of units and components were performed during integration testing, which incorporated direct usage of these components.

The metric chosen to represent unit testing was test coverage. Test coverage is the percentage of code blocks covered by corresponding tests. All the unit tests passed validation, hence it was somewhat irrelevant to include a per-unit table of tests in this section. It was more descriptive to see how much of the code base is covered by unit tests. The actual execution of the unit tests were automated using Jenkins(\ref{subsec:prestudies-tools-jenkins}), in combination with its TestNG plugin(\ref{subsec:prestudies-tools-testng}).

The following list provides an overview of the different packages in the product and their test coverage.

\begin{enumerate}
\item core - ??\%
\item core.event - ??\%
\item core.messaging - ??\%
\item core.subscription - ??\%
\item core.topic - ??\%
\item db - ??\%
\item examples - ??\%
\item exceptions - ??\%
\item protocol.wsn - ??\%
\item protocol.amqp - ??\%
\item web - ??\%
\end{enumerate}

The source code, including all the test methods can be found in the "test" package of the application.

\subsection{Integration testing}
\label{subsec:testing-test_execution-integration_testing}

The strategy used for integration testing was comprised of two parts. The first was to manually test the methods of the services and protocol modules. This first part was aimed at detecting problems with the internal dynamics of a service. A comprehensive list of test cases would not be appropriate to list in this document, as each component and service provides vastly different methods, yielding a large table. In short, all internal methods and components supporting a service's public functionality were tested manually.

The second part was to test all the exposed methods of each service and protocol module during normal system operation. This was due to the services needing the methods of other services to perform some of the actions. In an effort to create appropriate test cases for the services and protocol modules, the best way to uncover any potential flaws was to test every service itself, as well as all other services it communicates with. In practice, this meant starting the application, and using either a pre-written test function, or the admin interface to invoke the service method in question.

Using this strategy, the group regularly performed integration testing as new features were added to the services, and new aspects of the protocol standards were implemented.

This resulted in tables \ref{table:integration-testing-cases} to \ref{table:integration-testing-cases-messageservice} of test cases and their pass criteria, listed in Appendix \ref{ch:tables_and_graphs}. All tests were performed under normal system operation.

\subsection{System testing}
\label{subsec:testing-test_execution-system_testing}

System testing was performed based on the requirements for the system. Each of the functional requirements were tested in a procedural manner. Additionally, although no clear requirement were set for performance, it was tested to make sure there were no bottlenecks, and the system was performing fairly well. A table was made for each of the requirements.

\subsection{Acceptance test}
\label{subsec:testing-test_execution-acceptance_test}

A final demonstration of the system was performed at a customer meeting the 8th of May at NTNU. As previously stated, the customer did not have a strictly defined set of requirements for the product. This meant that the demonstration acted as our acceptance test. The group demonstrated the implementation of the customers requirements, as well as additional features and internal requirements from the requirements elicitation process. Due to time limitations, it was not possible to demonstrate all the different permutations of functionality acting together, as the customer had a second group meeting later that day. Nonetheless, the feedback given from the customer was very positive. There was some discussion regarding limitations of the WS-Nu library, as well as compatibility issues regarding translating messages between WSN and other non-XML protocols. Apart from one minor request for supporting topic based relaying between instances of the product, the product as a whole was accepted by the customer. Documentation of the limitations of WS-Nu and the challenges regarding message translation were also requested. A summary of the feedback is presented below:

\begin{itemize}
\item Having a relay function that can forward messages of certain topics to other instances of the system.
\item Support of WS-BrokeredNotification\footnote{\url{http://docs.oasis-open.org/wsn/wsn-ws_brokered_notification-1.3-spec-os.pdf}} and WS-BaseNotification\footnote{\url{http://docs.oasis-open.org/wsn/wsn-ws_base_notification-1.3-spec-os.pdf}}, it was important that both were tested and documented.
\item Document the research done on MQTT and MQTT libraries for further development.
\item The general functionality and purpose of the system were met.
\item The interface was intuitive, and the functionality was sufficient and good.
\end{itemize}

These factors were taken into consideration, and the few issues were assessed towards the end.



\clearpage