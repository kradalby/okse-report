%===================================== CHAP 6 =================================

\chapter{Testing}
\label{ch:testing}

\section{Test description}
\label{sec:testing-test_description}

The sections below cover the different test phases. It explains the different tests, as well as discusses the reasoning and usage of the different test methodologies.

\subsection{Unit Testing}
\label{subsec:testing-test_description-unit_testing}

A unit is a portion of source code, sets of one or more computer program modules together with associated control data, usage procedures, and operating procedures. A unit test is a specific test written to test the validity of said component or components. In Java, a unit is commonly defined as an entire class, but can also be a single object method. A unit test is written to confirm that the methods will act as intended, even if other parts of the program code is changed. Unit tests were used on the parts of the system containing many logical operations and calculations. Unit tests do not, however, exclude the possibility of errors. It only reduces the amount of cases where an error might occur.

\subsection{Integration testing}
\label{subsec:testing-test_description-integration_testing}

Integration testing is testing where individual modules of the software are grouped. It is a black-box test, in which major parts of the system are tested. The goal of the tests is to make sure that the major parts of the system are working as individual modules. It also ensures that modules are able to communicate with each other.

As each of the protocol implementations and core services of the system are standalone components that communicate with each other, integration testing is vital to the operation of the system. 

\subsection{Profiling}
\label{subsec:testing-test_description-profiling}

Profiling is a way to dynamically analyze the program while it is running. This is done to get an overview of where the resources are used. The method is useful to find bottlenecks and inefficient methods in the program. The software was run as a server instance, and had a possibility for high load on a wide variety of hardware. Profiling was useful to find the parts of the system that had performance issues. The system profiling was performed towards the end of the development period, as a part of the execution of system tests.

\subsection{System testing}
\label{subsec:testing-test_description-system_testing}

System testing is performed to ensure that the product meets its requirements. It is a black-box test, often performed upon completion of the product. The goal is to go through all the requirements, and ensure that they are met and working as intended. As a basis for the system testing, the group used several elements of the IEEE829 Test Plan Outline \cite{test-plan} as a template.

\subsection{Acceptance Testing}
\label{subsec:testing-test_description-acceptance_testing}
Acceptance testing is generally concerned with whether or not the requirements of the system are met in a satisfiable way. Since the system has a lot of functionality that is not visible, the acceptance test focused on testing the administration interface and to send and receive messages with the implemented applications.
As a precursor to the acceptance test, the group performed internal smoke tests \cite{smoke-test} of the system after every medium to big component was implemented. General aspects that were tested were requirements compliance, look and feel of the related user interface components, and double checking the interpretation of the customers stated wishes. The reasoning for the smoke tests was to be able to find errors that could impact the result of the final acceptance test.

\section{Test Execution}
\label{subsec:testing-test_execution}

The following section discusses how tests were planned and executed for each part of the system. The goal is to give a general overview of how tests were conducted and written.

\subsection{Unit testing}
\label{subsec:testing-test_execution-unit_testing}

Initially, the plan was to use test-driven development as often as possible, on the components that had unit test coverage. This proved somewhat difficult. In a lot of cases it was hard to know exactly which units were needed, and as the system evolved, the units changed in scope and behaviour. Thus, tests that were initially written to verify units, were changed later on. This was something the group was well aware of, and intentionally opted to discard TDD where the scope and behaviour of a unit was developed with a trial and error approach. However, that did not mean that unit tests were discarded as a whole. After those kind of components were finished, unit tests were added to verify their validity, as well as provide a safeguard for errors caused by future alterations.

Units of the system responsible for inter-thread communication, multi-thread task processing and queuing were excluded from unit tests. It proved difficult to write tests for these scenarios, as the group lacked the necessary time and experience to write unit tests for these conditions. The testing of those types of units and components were performed during integration testing, which incorporated direct usage of these components.

The metric chosen to represent unit testing was test coverage. Test coverage is the percentage of code blocks covered by corresponding tests. All the unit tests passed validation, hence it was somewhat irrelevant to include a per-unit table of tests in this section. It was more descriptive to see how much of the code base was covered by unit tests. The actual execution of the unit tests were automated using Jenkins (see section \ref{subsec:prestudies-tools-jenkins}), in combination with its TestNG plugin (see section \ref{subsec:prestudies-tools-testng}).
The following list provides an overview of the different packages in the product and their test coverage.


\begin{center}
\begin{table}[ht!]
\small
\centering
\begin{tabular}{|p{5cm}|p{2cm}|}
\hline
\rowcolor{lightgray}
\textbf{Name} & \textbf{Status} \\
\hline
 core & 60\%  \\
 core.event & 85\% \\ 
 core.messaging & 57\% \\ 
 core.subscription & 40\% \\ 
 core.topic & 74\% \\
 db & 56\% \\
 protocol.wsn & 37\% \\
 protocol.amqp & 78\% \\
 web & 0\% \\
\hline
\end{tabular}
\caption{Test coverage}
\label{tab:test coverage}
\end{table}
\end{center}


The source code, including all the test methods, can be found in the "test" package of the application.

\subsection{Integration testing}
\label{subsec:testing-test_execution-integration_testing}

The strategy used for integration testing was comprised of two parts. The first was to manually test the methods of the services and protocol modules. This first part was aimed at detecting problems with the internal dynamics of a service. A comprehensive list of test cases would not be appropriate to list in this document, as each component and service provides vastly different methods, yielding a large table. In short, all internal methods and components supporting a service's public functionality were tested manually.

The second part was to test all the exposed methods of each service and protocol module during normal system operation. This was due to the services needing the methods of other services to perform some of the actions. In an effort to create appropriate test cases for the services and protocol modules, the best way to uncover any potential flaws was to test every service itself, as well as all other services it communicated with. In practice, this meant starting the application, and using either a pre-written test function, or the admin interface to invoke the service method in question.

Using this strategy, the group regularly performed integration testing as new features were added to the services, and new aspects of the protocol standards were implemented. This resulted in tables \ref{table:integration-testing-cases} to \ref{table:integration-testing-cases-messageservice} of test cases and their pass criteria, listed in appendix \ref{ch:tables_and_graphs}. All tests were performed under normal system operation.

\subsection{System testing}
\label{subsec:testing-test_execution-system_testing}

The group assembled a system test plan based on the IEEE829 Test Plan outline, although excluding several sections. The below sections form the scope of the final system test plan as it was executed.

\subsubsection{Introduction}

This was the main system test plan. The plan covers the system test for all the main requirements, core services, protocol servers and support features of the OKSE Message Broker. It contains one level of testing, namely system testing.

\subsubsection{Test items}

The group assessed the functional requirements and the non-functional requirement of the system. A complete list of higher level test cases that were not directly covered by integration tests was devised. Additionally, an exhaustive list of test cases covering special cases, support features and profiling was created. The test cases for the defined requirements are found in tables \ref{table:system-testing-cases-fr1} through \ref{table:system-testing-cases-fr6} listed in appendix \ref{ch:tables_and_graphs}. The additional system test cases are listed in table \ref{table:system_testing-additional_test_cases} in appendix \ref{ch:tables_and_graphs}.

\subsubsection{Software risk issues}

There are several parts of the OKSE Message Broker that were not directly in control of the group. It was therefore important that edge cases involving support features from the 3rd party libraries used were assessed as well.\\
\\
The following risks affected the choice of test cases for the system test:

\begin{enumerate}[A.]
\item \textbf{WS-Nu and WSN standard compliance:} WSN support was the main concern of the customer, and as the protocol is the NATO standard, it holds the highest level of impact if defects are uncovered. Full standard support was desired.
\item \textbf{Qpid Proton:} The encoding of messages to be sent across the AMQP protocol needed preestimation of byte array size and enumeration until actual size is matched. This could cause a major performance hit on the AMQP protocol, and required profiling tests.
\item \textbf{Customer operational needs:} As the finished product will be used during military operational testing, support features like topic mapping and relay functionality is important for the product. Additionally, working initialization and setup of configuration directories and files are paramount to the effectiveness and features of the product.
\item \textbf{Core broker functionality.} The main task of the OKSE Message Broker is to act as a relay and translator for messages. Failure of the system to perform these core tasks would have devastating impact on the effectiveness of the product.
\end{enumerate}

\subsubsection{Components and features not to be tested}

To reduce the amount of resources needed to perform the testing, some components and features were not tested during the system test. Partially because they are covered by unit or integration tests, or that they have too small related risk to the effectiveness and operation of the product.
These are the components and features that were not covered by the system test:

\begin{enumerate}[A.]
\item The Log View pane in the administration interface, and its related controllers.
\item Support classes and components covered by unit tests and integration tests
\item Hierarchy based topic messaging. The OKSE Message Broker fully supports topic hierarchy aware messaging. Simply put, it means that subscribers to a root topic receive all messages from all children. However, none of the implemented protocols use this semantic. Additionally, the core methods of this functionality are well covered by unit tests, and was therefore deemed negligible for the system test.
\end{enumerate}

\subsubsection{Approach}

The execution of the system test would be managed by the group member responsible for testing and configuration management, with support from the lead developer and involved team members. All the tests were to be performed during normal system operation. Deliverables after execution were the test results. They are included in the complete system test case table in appendix \ref{ch:tables_and_graphs}. \\
\\
\textit{Configuration management}
\\
\\
The following configurations of the system were to be tested:
\begin{enumerate}[A.]
\item A clean installation with default configuration
\item An installation with custom hosts and ports set for the protocol servers, as well as the \verb!WSN_USE_WAN! flag set to true.
\item An installation with the \verb!AMQP_USE_QUEUE! flag set to true.
\end{enumerate}

\subsubsection{Pass criteria and results}

The complete listings of pass criteria for each test case and their respective results are presented in tables \ref{table:system-testing-cases-fr1} through \ref{table:system_testing-additional_test_cases} in appendix \ref{ch:tables_and_graphs}.

\subsection{Acceptance test}
\label{subsec:testing-test_execution-acceptance_test}

A final demonstration of the system was performed at a customer meeting on May 8 at NTNU. As previously stated, the customer did not have a strictly defined set of requirements for the product. This meant that the demonstration acted as our acceptance test. The group demonstrated the implementation of the customers requirements, as well as additional features and internal requirements from the requirements elicitation process. Due to time limitations, it was not possible to demonstrate all the different permutations of functionality acting together, as the customer had another meeting later that day. Nonetheless, the feedback given from the customer was very positive. There was some discussion regarding limitations of the WS-Nu library, as well as compatibility issues regarding translation of messages between WSN and other non-XML based protocols. Apart from one minor request for supporting topic based relaying between different instances of the product, the product as a whole was accepted by the customer. Documentation of the limitations of WS-Nu and the challenges regarding message translation were also requested. A summary of the feedback is presented below:

\begin{itemize}
\item Having a relay function that can forward messages of certain topics to other instances of the system.
\item Support of WS-BrokeredNotification \cite{wsn-brokerednotification} and WS-BaseNotification, it was important that both were tested and documented.
\item Document the research done on MQTT and MQTT libraries for further development.
\item All the general requirements and expectations of the system were met.
\item The interface was intuitive, and the functionality was sufficient and good.
\item The name was approved, and kept for release.
\end{itemize}

These factors were taken into consideration, and these few issues were assessed prior or during the final phase of the project.

\clearpage